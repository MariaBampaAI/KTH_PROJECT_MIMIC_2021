{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af87af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de5b7ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "917e20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# fix the random seed for tensorflow models\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1' \n",
    "SEED = 39\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# set to use flexible GPU resources  \n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c60bc8d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available: 0.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8213d6d1",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac61ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"./icd_demos_vitals.csv\")\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Maria\\\\Desktop\\\\data\\\\icd_demos_vitals.csv\")\n",
    "\n",
    "data.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58ed49fd",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        hadm_id  subject_id  mortality  HeartRate  SysBP  DiasBP  MeanBP  \\\n",
       "0      100061.0     11728.0        1.0       72.0  103.0    66.0    75.0   \n",
       "1      100061.0     11728.0        1.0       75.0   93.0    64.0    72.0   \n",
       "2      100061.0     11728.0        1.0       73.0  108.0    61.0    71.0   \n",
       "3      100061.0     11728.0        1.0       75.0  107.0    59.0    71.0   \n",
       "4      100061.0     11728.0        1.0       79.0  118.0    86.0    91.0   \n",
       "...         ...         ...        ...        ...    ...     ...     ...   \n",
       "53131  199984.0     55617.0        0.0       81.0  103.0    54.0    67.0   \n",
       "53132  199984.0     55617.0        0.0       79.0  111.0    52.0    65.0   \n",
       "53133  199984.0     55617.0        0.0       74.0  119.0    62.0    75.0   \n",
       "53134  199984.0     55617.0        0.0       81.0  121.0    63.0    78.0   \n",
       "53135  199984.0     55617.0        0.0       82.0  105.0    75.0    82.0   \n",
       "\n",
       "       RespRate      TempC  SpO2  ...   18   19   20    F    M  18-25  25-45  \\\n",
       "0          10.0   0.000000  93.0  ...  0.0  3.0  0.0  1.0  0.0    0.0    0.0   \n",
       "1           9.0   0.000000  91.0  ...  0.0  3.0  0.0  1.0  0.0    0.0    0.0   \n",
       "2          10.0   0.000000  94.0  ...  0.0  3.0  0.0  1.0  0.0    0.0    0.0   \n",
       "3          18.0   0.000000  95.0  ...  0.0  3.0  0.0  1.0  0.0    0.0    0.0   \n",
       "4          16.0   0.000000  95.0  ...  0.0  3.0  0.0  1.0  0.0    0.0    0.0   \n",
       "...         ...        ...   ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "53131      17.0  36.777778  94.0  ...  0.0  1.0  0.0  1.0  0.0    0.0    0.0   \n",
       "53132      19.0   0.000000   0.0  ...  0.0  1.0  0.0  1.0  0.0    0.0    0.0   \n",
       "53133      23.0   0.000000   0.0  ...  0.0  1.0  0.0  1.0  0.0    0.0    0.0   \n",
       "53134      16.0   0.000000   0.0  ...  0.0  1.0  0.0  1.0  0.0    0.0    0.0   \n",
       "53135      18.0  36.888889  96.0  ...  0.0  1.0  0.0  1.0  0.0    0.0    0.0   \n",
       "\n",
       "       45-65  65-89  89+  \n",
       "0        1.0    0.0  0.0  \n",
       "1        1.0    0.0  0.0  \n",
       "2        1.0    0.0  0.0  \n",
       "3        1.0    0.0  0.0  \n",
       "4        1.0    0.0  0.0  \n",
       "...      ...    ...  ...  \n",
       "53131    0.0    0.0  1.0  \n",
       "53132    0.0    0.0  1.0  \n",
       "53133    0.0    0.0  1.0  \n",
       "53134    0.0    0.0  1.0  \n",
       "53135    0.0    0.0  1.0  \n",
       "\n",
       "[53136 rows x 39 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hadm_id</th>\n      <th>subject_id</th>\n      <th>mortality</th>\n      <th>HeartRate</th>\n      <th>SysBP</th>\n      <th>DiasBP</th>\n      <th>MeanBP</th>\n      <th>RespRate</th>\n      <th>TempC</th>\n      <th>SpO2</th>\n      <th>...</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>F</th>\n      <th>M</th>\n      <th>18-25</th>\n      <th>25-45</th>\n      <th>45-65</th>\n      <th>65-89</th>\n      <th>89+</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100061.0</td>\n      <td>11728.0</td>\n      <td>1.0</td>\n      <td>72.0</td>\n      <td>103.0</td>\n      <td>66.0</td>\n      <td>75.0</td>\n      <td>10.0</td>\n      <td>0.000000</td>\n      <td>93.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100061.0</td>\n      <td>11728.0</td>\n      <td>1.0</td>\n      <td>75.0</td>\n      <td>93.0</td>\n      <td>64.0</td>\n      <td>72.0</td>\n      <td>9.0</td>\n      <td>0.000000</td>\n      <td>91.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100061.0</td>\n      <td>11728.0</td>\n      <td>1.0</td>\n      <td>73.0</td>\n      <td>108.0</td>\n      <td>61.0</td>\n      <td>71.0</td>\n      <td>10.0</td>\n      <td>0.000000</td>\n      <td>94.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100061.0</td>\n      <td>11728.0</td>\n      <td>1.0</td>\n      <td>75.0</td>\n      <td>107.0</td>\n      <td>59.0</td>\n      <td>71.0</td>\n      <td>18.0</td>\n      <td>0.000000</td>\n      <td>95.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100061.0</td>\n      <td>11728.0</td>\n      <td>1.0</td>\n      <td>79.0</td>\n      <td>118.0</td>\n      <td>86.0</td>\n      <td>91.0</td>\n      <td>16.0</td>\n      <td>0.000000</td>\n      <td>95.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>53131</th>\n      <td>199984.0</td>\n      <td>55617.0</td>\n      <td>0.0</td>\n      <td>81.0</td>\n      <td>103.0</td>\n      <td>54.0</td>\n      <td>67.0</td>\n      <td>17.0</td>\n      <td>36.777778</td>\n      <td>94.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>53132</th>\n      <td>199984.0</td>\n      <td>55617.0</td>\n      <td>0.0</td>\n      <td>79.0</td>\n      <td>111.0</td>\n      <td>52.0</td>\n      <td>65.0</td>\n      <td>19.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>53133</th>\n      <td>199984.0</td>\n      <td>55617.0</td>\n      <td>0.0</td>\n      <td>74.0</td>\n      <td>119.0</td>\n      <td>62.0</td>\n      <td>75.0</td>\n      <td>23.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>53134</th>\n      <td>199984.0</td>\n      <td>55617.0</td>\n      <td>0.0</td>\n      <td>81.0</td>\n      <td>121.0</td>\n      <td>63.0</td>\n      <td>78.0</td>\n      <td>16.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>53135</th>\n      <td>199984.0</td>\n      <td>55617.0</td>\n      <td>0.0</td>\n      <td>82.0</td>\n      <td>105.0</td>\n      <td>75.0</td>\n      <td>82.0</td>\n      <td>18.0</td>\n      <td>36.888889</td>\n      <td>96.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>53136 rows × 39 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "#either keep the patients with 24 hours of admission, or change 23 to sth smaller to include patients with less time steps\n",
    "df = lstm_preprocess.pad(data, 23, 24, 0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3dd6265b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['HeartRate',\n",
       " 'SysBP',\n",
       " 'DiasBP',\n",
       " 'MeanBP',\n",
       " 'RespRate',\n",
       " 'TempC',\n",
       " 'SpO2',\n",
       " 'Glucose',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " 'F',\n",
       " 'M',\n",
       " '18-25',\n",
       " '25-45',\n",
       " '45-65',\n",
       " '65-89',\n",
       " '89+']"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "#remove unnecessary columns\n",
    "COLUMNS = lstm_preprocess.delete_columns(df)\n",
    "\n",
    "COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7456312a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([100061., 100087., 100104., ..., 199976., 199981., 199984.])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "hadm_idx = df['hadm_id'].values.reshape(-1, 24, 1)[:, 0, 0]\n",
    "hadm_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5822043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the matrix to the appropriate format\n",
    "MATRIX = df[COLUMNS+['mortality']].values\n",
    "MATRIX = MATRIX.reshape(int(MATRIX.shape[0]/24),24,MATRIX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2eb729ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "bool_matrix = (~MATRIX.any(axis=2))\n",
    "MATRIX[bool_matrix] = np.nan\n",
    "#MATRIX = lstm_preprocess.ZScoreNormalize(MATRIX)\n",
    "\n",
    "## restore 3D shape to boolmatrix for consistency\n",
    "bool_matrix = np.isnan(MATRIX)\n",
    "MATRIX[bool_matrix] = 0 \n",
    "   \n",
    "#permutation = np.random.permutation(MATRIX.shape[0])\n",
    "#MATRIX = MATRIX[permutation]\n",
    "#bool_matrix = bool_matrix[permutation]\n",
    "\n",
    "# X_MATRIX = MATRIX[:,:,0:-1]\n",
    "X_MATRIX = MATRIX[:,:,0:8] # only use the first 8 temporal features, ignoring demographic data for now\n",
    "Y_MATRIX = MATRIX[:,:,-1]\n",
    "sc = MinMaxScaler()\n",
    "#x_bool_matrix = bool_matrix[:,:,0:-1]\n",
    "#y_bool_matrix = bool_matrix[:,:,-1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_matrix = (~MATRIX.any(axis=2))\n",
    "MATRIX[bool_matrix] = np.nan\n",
    "#MATRIX = lstm_preprocess.ZScoreNormalize(MATRIX)\n",
    "\n",
    "## restore 3D shape to boolmatrix for consistency\n",
    "bool_matrix = np.isnan(MATRIX)\n",
    "MATRIX[bool_matrix] = 0 \n",
    "   \n",
    "#permutation = np.random.permutation(MATRIX.shape[0])\n",
    "#MATRIX = MATRIX[permutation]\n",
    "#bool_matrix = bool_matrix[permutation]\n",
    "\n",
    "#X_MATRIX = MATRIX[:,:,0:-7]\n",
    "X_MATRIX = MATRIX[:,:,0:-2]\n",
    "Y_MATRIX = MATRIX[:,:,-1]\n",
    "#x_bool_matrix = bool_matrix[:,:,0:-1]\n",
    "#y_bool_matrix = bool_matrix[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cda1548a",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0    1819\n",
       "1.0     395\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "pd.value_counts(Y_MATRIX[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "\n",
    "\n",
    "tt_split = 0.7 \n",
    "val_percentage = 0.8\n",
    "\n",
    "X_TRAIN = X_MATRIX[0:int(tt_split*X_MATRIX.shape[0]),:,:]\n",
    "Y_TRAIN = Y_MATRIX[0:int(tt_split*Y_MATRIX.shape[0]),:]\n",
    "Y_TRAIN = Y_TRAIN[:, 0] \n",
    "Y_TRAIN = Y_TRAIN.reshape(Y_TRAIN.shape[0], 1)\n",
    "#Y_TRAIN = Y_TRAIN.reshape(Y_TRAIN.shape[0], Y_TRAIN.shape[1], 1)\n",
    "\n",
    "X_VAL = X_MATRIX[int(tt_split*X_MATRIX.shape[0]):int(val_percentage*X_MATRIX.shape[0])]\n",
    "Y_VAL = Y_MATRIX[int(tt_split*Y_MATRIX.shape[0]):int(val_percentage*Y_MATRIX.shape[0])]\n",
    "Y_VAL = Y_VAL[:, 0] \n",
    "Y_VAL = Y_VAL.reshape(Y_VAL.shape[0], 1)\n",
    "\n",
    "X_TEST = X_MATRIX[int(val_percentage*X_MATRIX.shape[0])::]\n",
    "Y_TEST = Y_MATRIX[int(val_percentage*X_MATRIX.shape[0])::]\n",
    "Y_TEST = Y_TEST[:, 0] \n",
    "Y_TEST = Y_TEST.reshape(Y_TEST.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91b60380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, validation, test split\n",
    "tt_split = 0.7 \n",
    "val_percentage = 0.8\n",
    "\n",
    "train_tail_idx = int(tt_split*X_MATRIX.shape[0])\n",
    "val_tail_idx = int(val_percentage*X_MATRIX.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30a18f95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the admission idx for train/validation/test, used to extract text patient data for prediction \n",
    "train_hadm_idx = hadm_idx[:train_tail_idx]\n",
    "val_hadm_idx = hadm_idx[train_tail_idx:val_tail_idx]\n",
    "test_hadm_idx = hadm_idx[val_tail_idx::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce6e05fc",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nx_test_boolmat = x_bool_matrix[int(val_percentage*x_bool_matrix.shape[0])::]\\ny_test_boolmat = y_bool_matrix[int(val_percentage*y_bool_matrix.shape[0])::]\\ny_test_boolmat = y_test_boolmat.reshape(y_test_boolmat.shape[0],y_test_boolmat.shape[1],1)\\n\\nX_TEST[x_test_boolmat] = 0\\nY_TEST[y_test_boolmat] = 0\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "X_TRAIN = X_MATRIX[0:train_tail_idx, :, :]\n",
    "Y_TRAIN = Y_MATRIX[0:train_tail_idx, 0]\n",
    "# Y_TRAIN = Y_TRAIN.reshape(Y_TRAIN.shape[0], Y_TRAIN.shape[1], 1)\n",
    "\n",
    "X_VAL = X_MATRIX[train_tail_idx:val_tail_idx]\n",
    "Y_VAL = Y_MATRIX[train_tail_idx:val_tail_idx, 0]\n",
    "# Y_VAL = Y_VAL.reshape(Y_VAL.shape[0], Y_VAL.shape[1], 1)\n",
    "\"\"\"\n",
    "x_val_boolmat = x_bool_matrix[int(tt_split*x_bool_matrix.shape[0]):int(val_percentage*x_bool_matrix.shape[0])]\n",
    "y_val_boolmat = y_bool_matrix[int(tt_split*y_bool_matrix.shape[0]):int(val_percentage*y_bool_matrix.shape[0])]\n",
    "y_val_boolmat = y_val_boolmat.reshape(y_val_boolmat.shape[0],y_val_boolmat.shape[1],1)\n",
    "\"\"\"\n",
    "X_TEST = X_MATRIX[val_tail_idx::]\n",
    "Y_TEST = Y_MATRIX[val_tail_idx::, 0]\n",
    "# Y_TEST = Y_TEST.reshape(Y_TEST.shape[0], Y_TEST.shape[1], 1)\n",
    "\"\"\"\n",
    "x_test_boolmat = x_bool_matrix[int(val_percentage*x_bool_matrix.shape[0])::]\n",
    "y_test_boolmat = y_bool_matrix[int(val_percentage*y_bool_matrix.shape[0])::]\n",
    "y_test_boolmat = y_test_boolmat.reshape(y_test_boolmat.shape[0],y_test_boolmat.shape[1],1)\n",
    "\n",
    "X_TEST[x_test_boolmat] = 0\n",
    "Y_TEST[y_test_boolmat] = 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f1f6b06",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "no_feature_cols = X_TRAIN.shape[2]\n",
    "no_feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape should be dx1:  (34,)\n",
      "c:\\Users\\Maria\\Desktop\\Projects Data Scripts\\KTH_PROJECT_MIMIC_2021\\data_extraction\\lstm_preprocess.py:76: RuntimeWarning: invalid value encountered in true_divide\n",
      "  train /= train_std\n",
      "c:\\Users\\Maria\\Desktop\\Projects Data Scripts\\KTH_PROJECT_MIMIC_2021\\data_extraction\\lstm_preprocess.py:79: RuntimeWarning: invalid value encountered in true_divide\n",
      "  val /= train_std\n",
      "c:\\Users\\Maria\\Desktop\\Projects Data Scripts\\KTH_PROJECT_MIMIC_2021\\data_extraction\\lstm_preprocess.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "  test /= train_std\n"
     ]
    }
   ],
   "source": [
    "X_TRAIN, X_VAL, X_TEST = lstm_preprocess.normalize(X_TRAIN, X_TEST, X_VAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "#need to reomove nan values,\n",
    "np.isnan(X_TRAIN).any()\n",
    "\n",
    "\n",
    "bool_matrix = np.isnan(X_VAL)\n",
    "X_VAL[bool_matrix] = 0 \n",
    "np.isnan(X_VAL).any()\n",
    "\n",
    "bool_matrix = np.isnan(X_TRAIN)\n",
    "X_TRAIN[bool_matrix] = 0 \n",
    "np.isnan(X_TRAIN).any()\n",
    "\n",
    "bool_matrix = np.isnan(X_TEST)\n",
    "X_TEST[bool_matrix] = 0 \n",
    "np.isnan(X_TEST).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c023c9e",
   "metadata": {},
   "source": [
    "## Load text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8bf1fb3a",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  subject_id   hadm_id  \\\n",
       "0  sinus tachycardia delayed precordial r wave tr...       77067  140990.0   \n",
       "1  normal sinus rhythm poor r wave progression po...       40304  174997.0   \n",
       "2  sinus rhythm probable prior inferior myocardia...       80932  190053.0   \n",
       "3  sinus rhythm left ventricular hypertrophy seco...       92752  138578.0   \n",
       "4  sinus rhythm marked right axis deviation possi...       18268  128217.0   \n",
       "\n",
       "   mortality  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>subject_id</th>\n      <th>hadm_id</th>\n      <th>mortality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sinus tachycardia delayed precordial r wave tr...</td>\n      <td>77067</td>\n      <td>140990.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>normal sinus rhythm poor r wave progression po...</td>\n      <td>40304</td>\n      <td>174997.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sinus rhythm probable prior inferior myocardia...</td>\n      <td>80932</td>\n      <td>190053.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sinus rhythm left ventricular hypertrophy seco...</td>\n      <td>92752</td>\n      <td>138578.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sinus rhythm marked right axis deviation possi...</td>\n      <td>18268</td>\n      <td>128217.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "text_data = pd.read_csv('C:\\\\Users\\\\Maria\\\\Desktop\\\\data\\\\texts.csv')\n",
    "text_data.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "text_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3a47169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = text_data['text'], text_data['mortality']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=39, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cdcbb739",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = text_data[text_data['hadm_id'].isin(set(train_hadm_idx))]\n",
    "val_data = text_data[text_data['hadm_id'].isin(set(val_hadm_idx))]\n",
    "test_data = text_data[text_data['hadm_id'].isin(set(test_hadm_idx))]\n",
    "\n",
    "X_train, y_train = train_data['text'],  train_data['mortality']\n",
    "X_val, y_val = val_data['text'],  val_data['mortality']\n",
    "X_test, y_test = test_data['text'],  test_data['mortality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "61af9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the joint hadm ids for train/test/val\n",
    "join_train_hadm_idx = np.unique(train_data['hadm_id'])\n",
    "join_val_hadm_idx = np.unique(val_data['hadm_id'])\n",
    "join_test_hadm_idx = np.unique(test_data['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f2f691a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the time series data to have same hadm id as text data\n",
    "X_TRAIN = X_TRAIN[np.in1d(train_hadm_idx, join_train_hadm_idx)]\n",
    "Y_TRAIN = Y_TRAIN[np.in1d(train_hadm_idx, join_train_hadm_idx)]\n",
    "\n",
    "X_VAL = X_VAL[np.in1d(val_hadm_idx, join_val_hadm_idx)]\n",
    "Y_VAL = Y_VAL[np.in1d(val_hadm_idx, join_val_hadm_idx)]\n",
    "\n",
    "X_TEST = X_TEST[np.in1d(test_hadm_idx, join_test_hadm_idx)]\n",
    "Y_TEST = Y_TEST[np.in1d(test_hadm_idx, join_test_hadm_idx)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cdd07b71",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1529,)"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe43112c",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1529, 24, 34)"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "X_TRAIN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0fe64692",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 3000\n",
    "\n",
    "# Tokenize the train text\n",
    "train_text = X_train.to_numpy()\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=NUM_WORDS, \n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "    char_level=False,\n",
    "    oov_token='<unk>',\n",
    "    document_count=0\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7b637230",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1000\n",
    "\n",
    "train_seqs = tokenizer.texts_to_sequences(train_text)\n",
    "train_seqs = keras.preprocessing.sequence.pad_sequences(train_seqs, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "train_labels = y_train.to_numpy().flatten()\n",
    "\n",
    "valid_text = X_val.to_numpy()\n",
    "valid_seqs = tokenizer.texts_to_sequences(valid_text)\n",
    "valid_seqs = keras.preprocessing.sequence.pad_sequences(valid_seqs, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "valid_labels = y_val.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe1ffa16",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[   4,   79,    8, ...,    7,  133,  198],\n",
       "       [   1,  787,   10, ...,   62,  344,  784],\n",
       "       [ 881, 1460,  269, ...,  720,  312,  689],\n",
       "       ...,\n",
       "       [  82,   91,  197, ...,  973,   60,    1],\n",
       "       [1073,  935, 1424, ...,   13, 1243, 1111],\n",
       "       [   6,   30, 2978, ...,  754, 1373,  153]])"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "train_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9361b2da",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1529, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "train_seqs.shape\n",
    "\n",
    "# (30455, 606752) without preprocessing/limiting and truncating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b088e",
   "metadata": {},
   "source": [
    "## Use the composite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95574175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompositeModel(n_timesteps, n_features, text_input_size): # n_features2=1 for text data\n",
    "    # classifier 1 (for time series):\n",
    "    inputs1 = keras.Input(shape=(n_timesteps, n_features))\n",
    "    output1 = keras.layers.LSTM(64, activation='tanh')(inputs1)\n",
    "\n",
    "    classifier1 = keras.Model(inputs1, output1, name=\"classifier1\")\n",
    "\n",
    "    # classifier 2 (for text data)\n",
    "    inputs2 = keras.Input(text_input_size,)\n",
    "    x = keras.layers.Embedding(input_dim=NUM_WORDS, output_dim=64, input_length=text_input_size)(inputs2)\n",
    "    output2 = keras.layers.LSTM(64, activation='tanh')(x)\n",
    "\n",
    "    classifier2 = keras.models.Model(inputs2, output2, name=\"classifier2\")\n",
    "\n",
    "    # final prediction\n",
    "    combined = keras.layers.concatenate([classifier1.output, classifier2.output])\n",
    "    # combined outputs\n",
    "    x = keras.layers.Dense(2, activation=\"relu\")(combined)\n",
    "    outputs3 = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    composite_model = keras.models.Model([classifier1.input, classifier2.input], outputs3)\n",
    "    \n",
    "    return composite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ca85ce93",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 24, 34)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 64)     192000      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 64)           25344       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 64)           33024       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            3           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 250,629\n",
      "Trainable params: 250,629\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "c:\\Users\\Maria\\Desktop\\Projects Data Scripts\\KTH_PROJECT_MIMIC_2021\\.venv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "composite_model = CompositeModel(n_timesteps=24, n_features=no_feature_cols, text_input_size=MAX_LEN)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "composite_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "composite_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "16098a16",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "#TODO: plot the model structure\n",
    "keras.utils.plot_model(composite_model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "14928533",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1529, 24, 34)"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "X_TRAIN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c51f919e",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1529, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "train_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9810a92e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Pip module Unable to parse debugpy output, please log an issue with https://github.com/microsoft/vscode-jupyter is required for debugging cells. You will need to install it to debug cells.",
     "traceback": [
      "Error: Pip module Unable to parse debugpy output, please log an issue with https://github.com/microsoft/vscode-jupyter is required for debugging cells. You will need to install it to debug cells.",
      "at _.parseConnectInfo (c:\\Users\\Maria\\.vscode\\extensions\\ms-toolsai.jupyter-2021.6.832593372\\out\\client\\extension.js:90:161460)",
      "at _.connectToLocal (c:\\Users\\Maria\\.vscode\\extensions\\ms-toolsai.jupyter-2021.6.832593372\\out\\client\\extension.js:90:161987)",
      "at async _.connect (c:\\Users\\Maria\\.vscode\\extensions\\ms-toolsai.jupyter-2021.6.832593372\\out\\client\\extension.js:90:159864)",
      "at async _.startDebugSession (c:\\Users\\Maria\\.vscode\\extensions\\ms-toolsai.jupyter-2021.6.832593372\\out\\client\\extension.js:90:159011)",
      "at async D.submitCode (c:\\Users\\Maria\\.vscode\\extensions\\ms-toolsai.jupyter-2021.6.832593372\\out\\client\\extension.js:37:622214)",
      "at async D.handleRunByLine (c:\\Users\\Maria\\.vscode\\extensions\\ms-toolsai.jupyter-2021.6.832593372\\out\\client\\extension.js:37:610158)"
     ]
    }
   ],
   "source": [
    "# Define the early stopping criteria\n",
    "early_stopping_accuracy = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True) # patient = 5 or 10 doesn't guarantee find an optimal\n",
    "\n",
    "# Train the model\n",
    "# reset_seeds()\n",
    "classifier_history2 = composite_model.fit([X_TRAIN, train_seqs], \n",
    "          y_train, \n",
    "          epochs=50,\n",
    "          batch_size=128,\n",
    "          shuffle=True, \n",
    "          verbose=True, \n",
    "          validation_data=([X_VAL, valid_seqs], y_val),\n",
    "          callbacks=[early_stopping_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5807a0b1",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8036529680365296\n          Pred:pos  Pred:neg\nTrue:pos         0        42\nTrue:neg         1       176\n              precision    recall  f1-score   support\n\n         0.0       0.81      0.99      0.89       177\n         1.0       0.00      0.00      0.00        42\n\n    accuracy                           0.80       219\n   macro avg       0.40      0.50      0.45       219\nweighted avg       0.65      0.80      0.72       219\n\n"
     ]
    }
   ],
   "source": [
    "y_pred = composite_model.predict([X_VAL, valid_seqs])\n",
    "y_pred_classes = np.array([1 if pred > 0.5 else 0 for pred in y_pred])\n",
    "\n",
    "# classification report\n",
    "acc = accuracy_score(y_true=Y_VAL, y_pred=y_pred_classes)\n",
    "print(acc)\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(\n",
    "        confusion_matrix(y_true=Y_VAL, y_pred=y_pred_classes, labels=[1, 0]),\n",
    "        index=['True:pos', 'True:neg'], \n",
    "        columns=['Pred:pos', 'Pred:neg']\n",
    "    )\n",
    "print(confusion_matrix_df)\n",
    "\n",
    "print(classification_report(y_true=Y_VAL, y_pred=y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = composite_model.evaluate(X_TRAIN, Y_TRAIN, batch_size=16, verbose=0)\n",
    "\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[   4,   79,    8, ...,    7,  133,  198],\n",
       "       [   1,  787,   10, ...,   62,  344,  784],\n",
       "       [ 881, 1460,  269, ...,  720,  312,  689],\n",
       "       ...,\n",
       "       [  82,   91,  197, ...,  973,   60,    1],\n",
       "       [1073,  935, 1424, ...,   13, 1243, 1111],\n",
       "       [   6,   30, 2978, ...,  754, 1373,  153]])"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "train_seqs"
   ]
  },
  {
   "source": [
    "### Create a model with bias initi and weights on class imbalance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       0\n",
       "3       0\n",
       "4       0\n",
       "7       0\n",
       "9       1\n",
       "       ..\n",
       "3572    0\n",
       "3573    1\n",
       "3576    0\n",
       "3580    0\n",
       "3581    1\n",
       "Name: mortality, Length: 1529, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.Series(list(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Examples:\n    Total: 1529\n    Positive: 273 (17.85% of total)\n\n"
     ]
    }
   ],
   "source": [
    "neg, pos = np.bincount(labels)\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Weight for class 0: 0.61\nWeight for class 1: 2.80\n"
     ]
    }
   ],
   "source": [
    "# you would want to have the classifier heavily weight the few examples that are available. You can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class.\n",
    "\n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-1.52621555])"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "initial_bias = np.log([pos/neg])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompositeModel(n_timesteps, n_features, text_input_size, output_bias=None): # n_features2=1 for text data\n",
    "    if output_bias is not None:\n",
    "        output_bias = tensorflow.keras.initializers.Constant(output_bias)\n",
    "    # classifier 1 (for time series):\n",
    "    inputs1 = keras.Input(shape=(n_timesteps, n_features))\n",
    "    output1 = keras.layers.LSTM(32, activation='tanh')(inputs1)\n",
    "\n",
    "    classifier1 = keras.Model(inputs1, output1, name=\"classifier1\")\n",
    "\n",
    "    # classifier 2 (for text data)\n",
    "    inputs2 = keras.Input(text_input_size,)\n",
    "    x = keras.layers.Embedding(input_dim=NUM_WORDS, output_dim=8, input_length=text_input_size)(inputs2)\n",
    "    output2 = keras.layers.LSTM(32, activation='tanh')(x)\n",
    "\n",
    "    classifier2 = keras.models.Model(inputs2, output2, name=\"classifier2\")\n",
    "\n",
    "    # final prediction\n",
    "    combined = keras.layers.concatenate([classifier1.output, classifier2.output])\n",
    "    # combined outputs\n",
    "    x = keras.layers.Dense(8, activation=\"relu\", bias_initializer = output_bias)(combined)\n",
    "    outputs3 = keras.layers.Dense(1, activation=\"sigmoid\", bias_initializer = output_bias)(x)\n",
    "\n",
    "    composite_model = keras.models.Model([classifier1.input, classifier2.input], outputs3)\n",
    "    \n",
    "    return composite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompositeModel(n_timesteps, n_features, text_input_size, initial_bias): # n_features2=1 for text data\n",
    "    # classifier 1 (for time series):\n",
    "    inputs1 = keras.Input(shape=(n_timesteps, n_features))\n",
    "    output1 = keras.layers.LSTM(32, activation='tanh', bias_regularizer= tf.keras.regularizers.L1L2(l1=0.01, l2=0.01))(inputs1)\n",
    "#     output1 = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    classifier1 = keras.Model(inputs1, output1, name=\"classifier1\")\n",
    "\n",
    "    # classifier 2 (for text data)\n",
    "    inputs2 = keras.Input(text_input_size,)\n",
    "    x = keras.layers.Embedding(input_dim=NUM_WORDS, output_dim=8, input_length=text_input_size)(inputs2)\n",
    "    output2 = keras.layers.LSTM(32, activation='tanh')(x)\n",
    "#     output2 = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    classifier2 = keras.models.Model(inputs2, output2, name=\"classifier2\")\n",
    "\n",
    "    # final prediction\n",
    "    combined = keras.layers.concatenate([classifier1.output, classifier2.output])\n",
    "    # combined outputs\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(combined)\n",
    "    outputs3 = keras.layers.Dense(1, activation=\"sigmoid\",\n",
    "                                 bias_initializer=keras.initializers.Constant(initial_bias))(x)\n",
    "\n",
    "    composite_model = keras.models.Model([classifier1.input, classifier2.input], outputs3)\n",
    "    \n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.001)\n",
    "    composite_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return composite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           [(None, 24, 34)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 1000, 8)      24000       input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_28 (LSTM)                  (None, 32)           8576        input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_29 (LSTM)                  (None, 32)           5248        embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 64)           0           lstm_28[0][0]                    \n",
      "                                                                 lstm_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 8)            520         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1)            9           dense_28[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 38,353\n",
      "Trainable params: 38,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "c:\\Users\\Maria\\Desktop\\Projects Data Scripts\\KTH_PROJECT_MIMIC_2021\\.venv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "composite_model = CompositeModel(\n",
    "    n_timesteps=24, n_features=no_feature_cols, text_input_size=MAX_LEN, initial_bias=initial_bias)\n",
    "\n",
    "composite_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "48/48 [==============================] - 22s 285ms/step - loss: 1.3767 - accuracy: 0.6050 - val_loss: 1.2694 - val_accuracy: 0.5479\n",
      "Epoch 2/50\n",
      "48/48 [==============================] - 12s 258ms/step - loss: 1.2593 - accuracy: 0.5356 - val_loss: 1.2553 - val_accuracy: 0.4292\n",
      "Epoch 3/50\n",
      "48/48 [==============================] - 13s 266ms/step - loss: 1.2019 - accuracy: 0.5978 - val_loss: 1.0573 - val_accuracy: 0.7671\n",
      "Epoch 4/50\n",
      "48/48 [==============================] - 13s 263ms/step - loss: 1.1205 - accuracy: 0.6710 - val_loss: 1.0636 - val_accuracy: 0.6484\n",
      "Epoch 5/50\n",
      "48/48 [==============================] - 12s 259ms/step - loss: 1.0052 - accuracy: 0.7711 - val_loss: 1.1510 - val_accuracy: 0.5479\n",
      "Epoch 6/50\n",
      "48/48 [==============================] - 12s 258ms/step - loss: 0.8848 - accuracy: 0.8332 - val_loss: 0.9673 - val_accuracy: 0.6941\n",
      "Epoch 7/50\n",
      "48/48 [==============================] - 12s 256ms/step - loss: 0.7846 - accuracy: 0.8430 - val_loss: 0.8655 - val_accuracy: 0.7671\n",
      "Epoch 8/50\n",
      "48/48 [==============================] - 12s 257ms/step - loss: 0.6935 - accuracy: 0.8803 - val_loss: 0.7966 - val_accuracy: 0.8037\n",
      "Epoch 9/50\n",
      "48/48 [==============================] - 12s 256ms/step - loss: 0.6377 - accuracy: 0.8986 - val_loss: 0.8161 - val_accuracy: 0.7808\n",
      "Epoch 10/50\n",
      "48/48 [==============================] - 12s 255ms/step - loss: 0.5485 - accuracy: 0.9169 - val_loss: 0.7367 - val_accuracy: 0.8219\n",
      "Epoch 11/50\n",
      "48/48 [==============================] - 12s 256ms/step - loss: 0.4952 - accuracy: 0.9372 - val_loss: 0.7287 - val_accuracy: 0.8037\n",
      "Epoch 12/50\n",
      "48/48 [==============================] - 13s 264ms/step - loss: 0.4190 - accuracy: 0.9398 - val_loss: 0.8744 - val_accuracy: 0.6712\n",
      "Epoch 13/50\n",
      "48/48 [==============================] - 13s 264ms/step - loss: 0.3898 - accuracy: 0.9313 - val_loss: 0.7021 - val_accuracy: 0.8174\n",
      "Epoch 14/50\n",
      "48/48 [==============================] - 13s 264ms/step - loss: 0.3093 - accuracy: 0.9581 - val_loss: 0.7362 - val_accuracy: 0.7854\n",
      "Epoch 15/50\n",
      "48/48 [==============================] - 13s 269ms/step - loss: 0.2505 - accuracy: 0.9686 - val_loss: 0.7132 - val_accuracy: 0.7945\n",
      "Epoch 16/50\n",
      "48/48 [==============================] - 13s 277ms/step - loss: 0.2008 - accuracy: 0.9732 - val_loss: 1.6599 - val_accuracy: 0.6256\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping criteria\n",
    "early_stopping_accuracy = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True) # patient = 5 or 10 doesn't guarantee find an optimal\n",
    "\n",
    "# Train the model\n",
    "# reset_seeds()\n",
    "classifier_history = composite_model.fit([X_TRAIN, train_seqs], \n",
    "          y_train, \n",
    "          epochs=50,\n",
    "          batch_size=32,\n",
    "          shuffle=True, \n",
    "          verbose=True, \n",
    "          validation_data=([X_VAL, valid_seqs], y_val),\n",
    "#           callbacks=[early_stopping_accuracy])\n",
    "          callbacks=[early_stopping_accuracy],\n",
    "          class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.771689497716895\nROC AUC SCORE Test\n0.5137207425343019\n          Pred:pos  Pred:neg\nTrue:pos         4        38\nTrue:neg        12       165\n              precision    recall  f1-score   support\n\n         0.0       0.81      0.93      0.87       177\n         1.0       0.25      0.10      0.14        42\n\n    accuracy                           0.77       219\n   macro avg       0.53      0.51      0.50       219\nweighted avg       0.70      0.77      0.73       219\n\n"
     ]
    }
   ],
   "source": [
    "y_pred = composite_model.predict([X_VAL, valid_seqs])\n",
    "y_pred_classes = np.array([1 if pred > 0.5 else 0 for pred in y_pred])\n",
    "\n",
    "# classification report\n",
    "acc = accuracy_score(y_true=Y_VAL, y_pred=y_pred_classes)\n",
    "print(acc)\n",
    "print('ROC AUC SCORE Test')\n",
    "print(roc_auc_score(Y_VAL,y_pred_classes))\n",
    "confusion_matrix_df = pd.DataFrame(\n",
    "        confusion_matrix(y_true=Y_VAL, y_pred=y_pred_classes, labels=[1, 0]),\n",
    "        index=['True:pos', 'True:neg'], \n",
    "        columns=['Pred:pos', 'Pred:neg']\n",
    "    )\n",
    "print(confusion_matrix_df)\n",
    "\n",
    "print(classification_report(y_true=Y_VAL, y_pred=y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "CompositeModel() missing 1 required positional argument: 'initial_bias'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-185-ca9f5c9c0abb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomposite_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCompositeModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_feature_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_input_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#optimizer = keras.optimizers.Adam(lr=0.0009)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-08\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcomposite_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: CompositeModel() missing 1 required positional argument: 'initial_bias'"
     ]
    }
   ],
   "source": [
    "composite_model = CompositeModel(n_timesteps=24, n_features=no_feature_cols, text_input_size=MAX_LEN)\n",
    "\n",
    "#optimizer = keras.optimizers.Adam(lr=0.0009)\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, epsilon=1e-08)\n",
    "composite_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "composite_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "#TODO: plot the model structure\n",
    "keras.utils.plot_model(composite_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 19s 4s/step - loss: 0.7130 - accuracy: 0.8182 - val_loss: 0.6048 - val_accuracy: 0.8037\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6994 - accuracy: 0.8162 - val_loss: 0.6166 - val_accuracy: 0.8128\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6910 - accuracy: 0.7986 - val_loss: 0.6295 - val_accuracy: 0.7671\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6818 - accuracy: 0.7698 - val_loss: 0.7786 - val_accuracy: 0.2557\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6771 - accuracy: 0.6056 - val_loss: 0.6419 - val_accuracy: 0.6941\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6644 - accuracy: 0.7142 - val_loss: 0.6884 - val_accuracy: 0.4977\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6582 - accuracy: 0.6913 - val_loss: 0.6882 - val_accuracy: 0.4977\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.6492 - accuracy: 0.7037 - val_loss: 0.7156 - val_accuracy: 0.4384\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 10s 4s/step - loss: 0.6464 - accuracy: 0.6756 - val_loss: 0.6536 - val_accuracy: 0.6073\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.6274 - accuracy: 0.6815 - val_loss: 0.6389 - val_accuracy: 0.6347\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 10s 4s/step - loss: 0.6082 - accuracy: 0.7345 - val_loss: 0.9190 - val_accuracy: 0.4155\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 10s 4s/step - loss: 0.6501 - accuracy: 0.5768 - val_loss: 0.5946 - val_accuracy: 0.7260\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping criteria\n",
    "early_stopping_accuracy = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True) # patient = 5 or 10 doesn't guarantee find an optimal\n",
    "\n",
    "# Train the model\n",
    "# reset_seeds()\n",
    "classifier_history2 = composite_model.fit([X_TRAIN, train_seqs], \n",
    "          y_train, \n",
    "          epochs=50,\n",
    "          batch_size=512,\n",
    "          shuffle=True, \n",
    "          verbose=True, \n",
    "          validation_data=([X_VAL, valid_seqs], y_val),\n",
    "          class_weight=class_weight,\n",
    "          callbacks=[early_stopping_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8036529680365296\nROC AUC SCORE Test\n0.5062550443906376\n          Pred:pos  Pred:neg\nTrue:pos         1        41\nTrue:neg         2       175\n              precision    recall  f1-score   support\n\n         0.0       0.81      0.99      0.89       177\n         1.0       0.33      0.02      0.04        42\n\n    accuracy                           0.80       219\n   macro avg       0.57      0.51      0.47       219\nweighted avg       0.72      0.80      0.73       219\n\n"
     ]
    }
   ],
   "source": [
    "y_pred = composite_model.predict([X_VAL, valid_seqs])\n",
    "y_pred_classes = np.array([1 if pred > 0.5 else 0 for pred in y_pred])\n",
    "\n",
    "# classification report\n",
    "acc = accuracy_score(y_true=Y_VAL, y_pred=y_pred_classes)\n",
    "print(acc)\n",
    "print('ROC AUC SCORE Test')\n",
    "print(roc_auc_score(Y_VAL,y_pred_classes))\n",
    "confusion_matrix_df = pd.DataFrame(\n",
    "        confusion_matrix(y_true=Y_VAL, y_pred=y_pred_classes, labels=[1, 0]),\n",
    "        index=['True:pos', 'True:neg'], \n",
    "        columns=['Pred:pos', 'Pred:neg']\n",
    "    )\n",
    "print(confusion_matrix_df)\n",
    "\n",
    "print(classification_report(y_true=Y_VAL, y_pred=y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonjvsc74a57bd01a45dd30e5671798ab819062d9e14f178a6c5e29c997e4a72a4b1344e290a839",
   "display_name": "Python 3.8.8  ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "1a45dd30e5671798ab819062d9e14f178a6c5e29c997e4a72a4b1344e290a839"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}